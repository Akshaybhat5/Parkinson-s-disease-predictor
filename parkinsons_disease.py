# -*- coding: utf-8 -*-
"""Parkinsons Disease.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lK6rax64CfBi5dJv_zfPihlbd5DVe3Zc

**PARKINSON'S DISEASE PREDICTOR**
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np


from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB

# extracting the dataset
data = pd.read_csv('parkinsons.csv')

data.head()

# Heat-map to understand the correlation between the variables
plt.figure(figsize=(15,15))
correlation = data.corr()
sns.heatmap(correlation, annot=True)
plt.title('HEAT-MAP', fontweight='bold')
plt.tight_layout()

# Name is not relevent when compared to other variables, so, let's remove Name from the dataframe
data.drop('name', axis=1, inplace=True)

data.head(10)

# let's rearrange the columns in the dataframe
data.columns

col = ['status','MDVP:Fo(Hz)', 'MDVP:Fhi(Hz)', 'MDVP:Flo(Hz)', 'MDVP:Jitter(%)',
       'MDVP:Jitter(Abs)', 'MDVP:RAP', 'MDVP:PPQ', 'Jitter:DDP',
       'MDVP:Shimmer', 'MDVP:Shimmer(dB)', 'Shimmer:APQ3', 'Shimmer:APQ5',
       'MDVP:APQ', 'Shimmer:DDA', 'NHR', 'HNR', 'RPDE', 'DFA',
       'spread1', 'spread2', 'D2', 'PPE']

re_arranged_data = data[col]

re_arranged_data.head()

re_arranged_data.info()

re_arranged_data.isnull().sum()

# Luckily none of the data is missing.

data['D2'].hist(bins=30, color='red')
plt.title('D2', fontweight='bold')
plt.tight_layout()

re_arranged_data.describe()

# checking, is there any outliers in certain variables?
plt.figure(figsize=(8,8))
sns.set_theme(style='darkgrid')
sns.distplot(re_arranged_data['MDVP:Fo(Hz)'])
plt.tight_layout()

# MIN-88
# MAX-260

# checking, is there any outliers in certain variables?
plt.figure(figsize=(8,8))
sns.distplot(re_arranged_data['MDVP:Fhi(Hz)'])
plt.tight_layout()

# MIN-102
# MAX-592

# checking, is there any outliers in certain variables?
plt.figure(figsize=(8,8))
sns.distplot(re_arranged_data['MDVP:Flo(Hz)'])
plt.tight_layout()

# MIN-65
# MAX-239

# checking, is there any outliers in certain variables?
plt.figure(figsize=(8,8))
sns.set_theme(style='darkgrid')
sns.distplot(re_arranged_data['HNR'])
plt.tight_layout()

# MIN-8
# MAX-33

# There is no outliers exist

# let's check for the vif assumptions
from statsmodels.stats.outliers_influence import variance_inflation_factor

# create variable for the vif
variable = re_arranged_data[['MDVP:Fo(Hz)', 'MDVP:Fhi(Hz)', 'MDVP:Flo(Hz)', 'MDVP:Jitter(%)',
       'MDVP:Jitter(Abs)', 'MDVP:RAP', 'MDVP:PPQ', 'Jitter:DDP',
       'MDVP:Shimmer', 'MDVP:Shimmer(dB)', 'Shimmer:APQ3', 'Shimmer:APQ5',
       'MDVP:APQ', 'Shimmer:DDA', 'NHR', 'HNR', 'RPDE', 'DFA',
       'spread1', 'spread2', 'D2', 'PPE']]

vif = pd.DataFrame()
vif['VIF'] = [variance_inflation_factor(variable.values, i) for i in range(variable.shape[1])]
vif['features'] = variable.columns
vif

# All the variables are important and relevent

preprocessed_data = re_arranged_data

y = preprocessed_data['status']
X = preprocessed_data.drop('status',axis=1)

y

X

# let's scale the data into standard form

from sklearn.preprocessing import StandardScaler
scalar = StandardScaler()
X = scalar.fit_transform(X)

X

# let's split the data into train test and split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=365)

X_train.shape, X_test.shape

# let's instantiate one by one model
log_regression = LogisticRegression(random_state=365)
log_regression.fit(X_train, y_train)

log_output = log_regression.predict(X_test)

log_output

# let's calculate the accuracy score, draw insights from confusion matrix
cm = confusion_matrix(y_test, log_output)
acc = accuracy_score(y_test, log_output)
cl = classification_report(y_test, log_output)

print(f'''The confusion metrix is: 
{cm}''')
print(f'''The accuracy score of the LOGISTIC REGRESSION IS: {round(acc, 6)*100}%''')
print(f'''The classification report is: 
{cl}''')

# let's instantiate Randomforest

random_forest = RandomForestClassifier(n_estimators=100, criterion='gini',random_state=365)
random_forest.fit(X_train, y_train)

out_random_forest = random_forest.predict(X_test)

out_random_forest

# let's calculate the accuracy score, draw insights from confusion matrix
cm = confusion_matrix(y_test, out_random_forest)
acc = accuracy_score(y_test, out_random_forest)
cl = classification_report(y_test, out_random_forest)

print(f'''The confusion metrix is: 
{cm}''')
print(f'''The accuracy score of the RANDOMFOREST CLASSIFIER IS: {round(acc, 6)*100}%''')
print(f'''The classification report is: 
{cl}''')

# let's instantiate decision tree classifier

tree = DecisionTreeClassifier()
tree.fit(X_train, y_train)

tree_output = tree.predict(X_test)

tree_output

# let's calculate the accuracy score, draw insights from confusion matrix
cm = confusion_matrix(y_test, tree_output)
acc = accuracy_score(y_test, tree_output)
cl = classification_report(y_test, tree_output)

print(f'''The confusion metrix is: 
{cm}''')
print(f'''The accuracy score of the DECISIONTREE CLASSIFIER IS: {round(acc, 6)*100}%''')
print(f'''The classification report is: 
{cl}''')

# let's instantiate naive baiyes

naive = GaussianNB()
naive.fit(X_train, y_train)

naive_out = naive.predict(X_test)

naive_out

# let's calculate the accuracy score, draw insights from confusion matrix
cm = confusion_matrix(y_test, naive_out)
acc = accuracy_score(y_test, naive_out)
cl = classification_report(y_test, naive_out)

print(f'''The confusion metrix is: 
{cm}''')
print(f'''The accuracy score of the GAUSSIAN NB IS: {round(acc, 6)*100}%''')
print(f'''The classification report is: 
{cl}''')

# let's instantiate KNeighbour classifier

k_neighbour = KNeighborsClassifier(n_neighbors=5, algorithm='auto',weights='uniform')
k_neighbour.fit(X_train, y_train)

k_neighbour_out = k_neighbour.predict(X_test)

k_neighbour_out

# let's calculate the accuracy score, draw insights from confusion matrix
cm = confusion_matrix(y_test, k_neighbour_out)
acc = accuracy_score(y_test, k_neighbour_out)
cl = classification_report(y_test, k_neighbour_out)

print(f'''The confusion metrix is: 
{cm}''')
print(f'''The accuracy score of the LOGISTIC REGRESSION IS: {round(acc, 6)*100}%''')
print(f'''The classification report is: 
{cl}''')

all_features = [['LOGISTIC REGRESSION', '92.30%'], ['KNEIGHBOR CLASSIFIER','89.74%'], ['GUASSIAN NB', '71.79%'],
                ['DECISIONTREE CLASSIFIER', '89.74%'], ['RANDOMFOREST CLASSIFIER', '92.30%']]

all_features

data_frame = pd.DataFrame(all_features, columns=['ML MODEL','ACCURACY SCORE'])

data_frame.sort_values(ascending=False, by='ACCURACY SCORE')

# function to find the maximum accuracy& ML MOdel in the dataframe
def max_finder(frame):
  for index, row in frame.iterrows():
    if row[1] >= '90%':
      print(row[0], row[1])

max_finder(data_frame)

# let's use randomforest model as it's better fits to the problem

import joblib
joblib.dump(random_forest, 'parkinson_disease_prediction_model.joblib')

